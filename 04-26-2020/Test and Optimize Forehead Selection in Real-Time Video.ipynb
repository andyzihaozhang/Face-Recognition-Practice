{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现视频实时人脸检测以及额头区域绘制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 方法（代码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5点面部特征"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# USAGE\n",
    "# python forehead_detector_5_point.py shape_predictor_5_face_landmarks.dat\n",
    "\n",
    "# import the necessary packages\n",
    "import sys\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "predictor_path = sys.argv[1]\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create the\n",
    "# facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "print(\"[INFO] loading HOG algorithm...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# initialize the video stream and sleep for a bit, allowing the\n",
    "# camera sensor to warm up\n",
    "vs = cv2.VideoCapture(0)\n",
    "\n",
    "# initialize the loop and fps list for average fps estimation\n",
    "loop = 0\n",
    "fps = []\n",
    "\n",
    "\n",
    "def get_forehead_coord(face_landmarks):\n",
    "    left = [face_landmarks.part(2).x, face_landmarks.part(2).y]\n",
    "    right = [face_landmarks.part(0).x, face_landmarks.part(0).y]\n",
    "    nose = [face_landmarks.part(4).x, face_landmarks.part(4).y]\n",
    "\n",
    "    eye_center = [(left[0]+right[0])/2, (left[1]+right[1])/2]\n",
    "    # distance between nose and eye center\n",
    "    dist1 = [nose[0]-eye_center[0], nose[1]-eye_center[1]]\n",
    "    # distance between eye center to left eye\n",
    "    dist2 = [eye_center[0]-left[0], eye_center[1]-left[1]]\n",
    "    bottom_center = [eye_center[0]-2/3*dist1[0], eye_center[1]-2/3*dist1[1]]\n",
    "    left_bottom = [bottom_center[0]-dist2[0], bottom_center[1]-dist2[1]]\n",
    "    right_bottom = [bottom_center[0]+dist2[0], bottom_center[1]+dist2[1]]\n",
    "    left_top = [left_bottom[0]-dist1[0], left_bottom[1]-dist1[1]]\n",
    "    right_top = [right_bottom[0]-dist1[0], right_bottom[1]-dist1[1]]\n",
    "\n",
    "    coord = np.array([left_bottom, right_bottom, right_top, left_top], np.int32)\n",
    "    coord = [coord.reshape((-1, 1, 2))]\n",
    "    return coord\n",
    "\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # grab the frame from the threaded video stream\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # recognize face every 5 frames, otherwise use most recent coordinates\n",
    "    if loop%5 == 0:\n",
    "        # convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # detect faces in the grayscale frame\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        # check to see if a face was detected, and if so, draw the total\n",
    "        # number of faces on the frame\n",
    "        if len(rects) > 0:\n",
    "            text = \"{} face(s) found\".format(len(rects))\n",
    "            cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (0, 0, 255), 2)\n",
    "\n",
    "        forehead_coord_list = []\n",
    "        # loop over the face detections\n",
    "        for rect in rects:\n",
    "            # compute the bounding box of the face and draw it on the frame\n",
    "            bX, bY, bW, bH = rect.left(), rect.top(), rect.right(), rect.bottom()\n",
    "            cv2.rectangle(frame, (bX, bY), (bW, bH), (0, 255, 0), 1)\n",
    "\n",
    "            # determine the facial landmarks for the face region\n",
    "            shape = predictor(gray, rect)\n",
    "\n",
    "            # store forehead coord for frame do not run face recognition\n",
    "            forehead_coord_list.append(get_forehead_coord(shape))\n",
    "\n",
    "            # draw the bounding box of the forehead\n",
    "            cv2.polylines(frame, get_forehead_coord(shape), True, (0, 255, 255))\n",
    "\n",
    "    else:\n",
    "        if len(rects) > 0:\n",
    "            text = \"{} face(s) found\".format(len(rects))\n",
    "            cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, (0, 0, 255), 2)\n",
    "\n",
    "        for rect in rects:\n",
    "            bX, bY, bW, bH = rect.left(), rect.top(), rect.right(), rect.bottom()\n",
    "            cv2.rectangle(frame, (bX, bY), (bW, bH), (0, 255, 0), 1)\n",
    "\n",
    "        for forehead_coord in forehead_coord_list:\n",
    "            cv2.polylines(frame, forehead_coord, True, (0, 255, 255))\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    loop += 1\n",
    "\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "\n",
    "    # calculate the fps and current frame and add it to fps list for\n",
    "    # average fps estimation\n",
    "    frame_fps = 1/(end - start)\n",
    "    fps.append(frame_fps)\n",
    "    \n",
    "    print(\"Average fps: \", sum(fps)/loop)\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 68点面部特征"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# USAGE\n",
    "# python forehead_detector_68_point.py shape_predictor_68_face_landmarks.dat\n",
    "\n",
    "# import the necessary packages\n",
    "import sys\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "predictor_path = sys.argv[1]\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create the\n",
    "# facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "print(\"[INFO] loading HOG algorithm...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# initialize the video stream and sleep for a bit, allowing the\n",
    "# camera sensor to warm up\n",
    "vs = cv2.VideoCapture(0)\n",
    "\n",
    "# initialize the loop and fps list for average fps estimation\n",
    "loop = 0\n",
    "fps = []\n",
    "\n",
    "\n",
    "def get_forehead_coord(face_landmarks):\n",
    "    left_bottom = [face_landmarks.part(17).x, face_landmarks.part(19).y]\n",
    "    right_bottom = [face_landmarks.part(26).x, face_landmarks.part(24).y]\n",
    "    \n",
    "    center_bottom = [(left_bottom[0]+right_bottom[0])/2, (left_bottom[1]+right_bottom[1])/2]\n",
    "    # distance between end of nose bridge and center of eyebrow\n",
    "    dist_1 = [face_landmarks.part(30).x - center_bottom[0], face_landmarks.part(30).y - center_bottom[1]]\n",
    "    center_top = [center_bottom[0]-dist_1[0], center_bottom[1]-dist_1[1]]\n",
    "    # distance between left/right bottom to the center of eyebrow\n",
    "    dist_2 = [center_bottom[0]-left_bottom[0], center_bottom[1]-left_bottom[1]]\n",
    "    left_top = [center_top[0]-dist_2[0], center_top[1]-dist_2[1]]\n",
    "    right_top = [center_top[0]+dist_2[0], center_top[1]+dist_2[1]]\n",
    "\n",
    "    coord = np.array([left_bottom, right_bottom, right_top, left_top], np.int32)\n",
    "    coord = [coord.reshape((-1,1,2))]\n",
    "    return coord\n",
    "\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # grab the frame from the threaded video stream\n",
    "    ret, frame = vs.read()\n",
    "\n",
    "    # recognize face every 5 frames, otherwise use most recent coordinates\n",
    "    if loop%5 == 0:\n",
    "        # convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # detect faces in the grayscale frame\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        # check to see if a face was detected, and if so, draw the total\n",
    "        # number of faces on the frame\n",
    "        if len(rects) > 0:\n",
    "            text = \"{} face(s) found\".format(len(rects))\n",
    "            cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (0, 0, 255), 2)\n",
    "\n",
    "        forehead_coord_list = []\n",
    "        # loop over the face detections\n",
    "        for rect in rects:\n",
    "            # compute the bounding box of the face and draw it on the frame\n",
    "            bX, bY, bW, bH = rect.left(), rect.top(), rect.right(), rect.bottom()\n",
    "            cv2.rectangle(frame, (bX, bY), (bW, bH), (0, 255, 0), 1)\n",
    "\n",
    "            # determine the facial landmarks for the face region\n",
    "            shape = predictor(gray, rect)\n",
    "\n",
    "            # store forehead coord for frame do not run face recognition\n",
    "            forehead_coord_list.append(get_forehead_coord(shape))\n",
    "            \n",
    "            # draw the bounding box of the forehead\n",
    "            cv2.polylines(frame, get_forehead_coord(shape), True, (0 ,255, 255))\n",
    "\n",
    "    else:\n",
    "        if len(rects) > 0:\n",
    "            text = \"{} face(s) found\".format(len(rects))\n",
    "            cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, (0, 0, 255), 2)\n",
    "\n",
    "        for rect in rects:\n",
    "            bX, bY, bW, bH = rect.left(), rect.top(), rect.right(), rect.bottom()\n",
    "            cv2.rectangle(frame, (bX, bY), (bW, bH), (0, 255, 0), 1)\n",
    "\n",
    "        for forehead_coord in forehead_coord_list:\n",
    "            cv2.polylines(frame, forehead_coord, True, (0, 255, 255))\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    loop += 1\n",
    "\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "\n",
    "    # calculate the fps and current frame and add it to fps list for\n",
    "    # average fps estimation\n",
    "    frame_fps = 1/(end - start)\n",
    "    fps.append(frame_fps)\n",
    "    \n",
    "    print(\"Average fps: \", sum(fps)/loop)\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 结果与思考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|              | HOG, 5 point | HOG, 68 point |\n",
    "|--------------|:------------:|:-------------:|\n",
    "| Memory Size  |    9.2 MB    |    99.7 MB    |\n",
    "| Max Distance |  ~ 2 meters  |  ~ 2 meters   |\n",
    "| Max Angle    | ~ 45 degrees | ~ 45 degrees  |\n",
    "| FPS          |    23 fps    |    22 fps     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. 5点面部特征模型占用硬盘内存相差有10倍，5点模型在内存容量较小的设备上有明显优势。\n",
    "2. 由于使用的面部识别模型同为HOG, 所有在最远识别距离和最大识别角度上没有差别。CNN模型由于对算力要求过高，在测试过程中由于出现明显卡顿（低帧率）被排除。\n",
    "3. 在测试帧率的时候，使用了每5帧检测一次的方法，使得帧率由原本的每秒9-10帧提高到了每秒23帧。在测试过程中，在目标没有快速移动的前提下，每5帧检测一次并没有使面部和额头的区域框发生显著的滞后。同时，5点和68点面部特征模型在运行效率上没有显著差别。\n",
    "4. 由于5点面部特征模型只提供了左眼两端、右眼两端和鼻尖坐标，故计算时适用了经验公式来估算额头区域，需要后期更多资料形成眼鼻位置与额头区域的准确关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "寻找每帧检测运行时间比dlib库更快的算法/库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 方法（代码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Ultra Light"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\"\"\"\n",
    "Required Packages and Versions:\n",
    "onnx==1.6.0\n",
    "onnx-tf==1.3.0\n",
    "onnxruntime==0.5.0\n",
    "opencv-python==4.1.1.26\n",
    "tensorflow==1.13.1\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "\n",
    "def area_of(left_top, right_bottom):\n",
    "    \"\"\"\n",
    "    Compute the areas of rectangles given two corners.\n",
    "    Args:\n",
    "        left_top (N, 2): left top corner.\n",
    "        right_bottom (N, 2): right bottom corner.\n",
    "    Returns:\n",
    "        area (N): return the area.\n",
    "    \"\"\"\n",
    "    hw = np.clip(right_bottom - left_top, 0.0, None)\n",
    "    return hw[..., 0] * hw[..., 1]\n",
    "\n",
    "\n",
    "def iou_of(boxes0, boxes1, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Args:\n",
    "        boxes0 (N, 4): ground truth boxes.\n",
    "        boxes1 (N or 1, 4): predicted boxes.\n",
    "        eps: a small number to avoid 0 as denominator.\n",
    "    Returns:\n",
    "        iou (N): IoU values.\n",
    "    \"\"\"\n",
    "    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
    "    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
    "\n",
    "    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
    "    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
    "    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
    "    return overlap_area / (area0 + area1 - overlap_area + eps)\n",
    "\n",
    "\n",
    "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
    "    \"\"\"\n",
    "    Perform hard non-maximum-supression to filter out boxes with iou greater\n",
    "    than threshold\n",
    "    Args:\n",
    "        box_scores (N, 5): boxes in corner-form and probabilities.\n",
    "        iou_threshold: intersection over union threshold.\n",
    "        top_k: keep top_k results. If k <= 0, keep all the results.\n",
    "        candidate_size: only consider the candidates with the highest scores.\n",
    "    Returns:\n",
    "        picked: a list of indexes of the kept boxes\n",
    "    \"\"\"\n",
    "    scores = box_scores[:, -1]\n",
    "    boxes = box_scores[:, :-1]\n",
    "    picked = []\n",
    "    indexes = np.argsort(scores)\n",
    "    indexes = indexes[-candidate_size:]\n",
    "    while len(indexes) > 0:\n",
    "        current = indexes[-1]\n",
    "        picked.append(current)\n",
    "        if 0 < top_k == len(picked) or len(indexes) == 1:\n",
    "            break\n",
    "        current_box = boxes[current, :]\n",
    "        indexes = indexes[:-1]\n",
    "        rest_boxes = boxes[indexes, :]\n",
    "        iou = iou_of(\n",
    "            rest_boxes,\n",
    "            np.expand_dims(current_box, axis=0),\n",
    "        )\n",
    "        indexes = indexes[iou <= iou_threshold]\n",
    "\n",
    "    return box_scores[picked, :]\n",
    "\n",
    "\n",
    "def predict(width, height, confidences, boxes, prob_threshold, iou_threshold=0.5, top_k=-1):\n",
    "    \"\"\"\n",
    "    Select boxes that contain human faces\n",
    "    Args:\n",
    "        width: original image width\n",
    "        height: original image height\n",
    "        confidences (N, 2): confidence array\n",
    "        boxes (N, 4): boxes array in corner-form\n",
    "        iou_threshold: intersection over union threshold.\n",
    "        top_k: keep top_k results. If k <= 0, keep all the results.\n",
    "    Returns:\n",
    "        boxes (k, 4): an array of boxes kept\n",
    "        labels (k): an array of labels for each boxes kept\n",
    "        probs (k): an array of probabilities for each boxes being in corresponding labels\n",
    "    \"\"\"\n",
    "    boxes = boxes[0]\n",
    "    confidences = confidences[0]\n",
    "    picked_box_probs = []\n",
    "    picked_labels = []\n",
    "    for class_index in range(1, confidences.shape[1]):\n",
    "        probs = confidences[:, class_index]\n",
    "        mask = probs > prob_threshold\n",
    "        probs = probs[mask]\n",
    "        if probs.shape[0] == 0:\n",
    "            continue\n",
    "        subset_boxes = boxes[mask, :]\n",
    "        box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n",
    "        box_probs = hard_nms(box_probs, iou_threshold=iou_threshold, top_k=top_k)\n",
    "        picked_box_probs.append(box_probs)\n",
    "        picked_labels.extend([class_index] * box_probs.shape[0])\n",
    "    if not picked_box_probs:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    picked_box_probs = np.concatenate(picked_box_probs)\n",
    "    picked_box_probs[:, 0] *= width\n",
    "    picked_box_probs[:, 1] *= height\n",
    "    picked_box_probs[:, 2] *= width\n",
    "    picked_box_probs[:, 3] *= height\n",
    "    return picked_box_probs[:, :4].astype(np.int32), np.array(picked_labels), picked_box_probs[:, 4]\n",
    "\n",
    "\n",
    "def get_forehead_coord(face_landmarks):\n",
    "    left_bottom = [face_landmarks.part(17).x, face_landmarks.part(19).y]\n",
    "    right_bottom = [face_landmarks.part(26).x, face_landmarks.part(24).y]\n",
    "\n",
    "    center_bottom = [(left_bottom[0] + right_bottom[0]) / 2, (left_bottom[1] + right_bottom[1]) / 2]\n",
    "    # distance between end of nose bridge and center of eyebrow\n",
    "    dist_1 = [face_landmarks.part(30).x - center_bottom[0], face_landmarks.part(30).y - center_bottom[1]]\n",
    "    center_top = [center_bottom[0] - dist_1[0], center_bottom[1] - dist_1[1]]\n",
    "    # distance between left/right bottom to the center of eyebrow\n",
    "    dist_2 = [center_bottom[0] - left_bottom[0], center_bottom[1] - left_bottom[1]]\n",
    "    left_top = [center_top[0] - dist_2[0], center_top[1] - dist_2[1]]\n",
    "    right_top = [center_top[0] + dist_2[0], center_top[1] + dist_2[1]]\n",
    "\n",
    "    coord = np.array([left_bottom, right_bottom, right_top, left_top], np.int32)\n",
    "    coord = [coord.reshape((-1, 1, 2))]\n",
    "    return coord\n",
    "\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "onnx_path = 'ultra_light_640.onnx'\n",
    "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "detector = prepare(onnx_model)\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "loop = 0\n",
    "fps = []\n",
    "\n",
    "while True:\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "\n",
    "    ret, frame = video_capture.read()\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # preprocess img acquired\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # convert bgr to rgb\n",
    "    img = cv2.resize(img, (640, 480))  # resize\n",
    "    img_mean = np.array([127, 127, 127])\n",
    "    img = (img - img_mean) / 128\n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    confidences, boxes = ort_session.run(None, {input_name: img})\n",
    "    boxes, labels, probs = predict(w, h, confidences, boxes, 0.7)\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        box = boxes[i, :]\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (80, 18, 236), 2)\n",
    "        cv2.rectangle(frame, (x1, y2 - 20), (x2, y2), (80, 18, 236), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        text = f\"face: {labels[i]}\"\n",
    "        cv2.putText(frame, text, (x1 + 6, y2 - 6), font, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        rect = dlib.rectangle(left=x1, top=y1, right=x2, bottom=y2)\n",
    "        shape = predictor(gray, rect)\n",
    "        cv2.polylines(frame, get_forehead_coord(shape), True, (0, 255, 255))\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    loop += 1\n",
    "\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "\n",
    "    # calculate the fps and current frame and add it to fps list for\n",
    "    # average fps estimation\n",
    "    frame_fps = 1 / (end - start)\n",
    "    fps.append(frame_fps)\n",
    "\n",
    "    print(\"Average fps: \", sum(fps) / loop)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果与思考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ultra Light模型运行的帧率有了明显提升，在不使用每5帧检测一次的方法时，帧率已经达到每秒14帧，相信在加入每n秒检测一次之后可以稳定输出流畅的视频。\n",
    "2. 与此同时，Ultra Light模型对于90度侧脸或距离摄像头4米时，依然保持较高的识别率，在这方面远优于HOG模型。\n",
    "3. 接下来还需要进行多人脸（10+）的测试以观测在多人脸时，HOG和Ultra Light的检测速度。\n",
    "4. 依据当前测试，在无口罩遮挡、普通RGB图像、实时视频、使用低内存、低算力设备进行人脸识别时，使用Ultra Light确定人脸位置和5点人脸特征模型确定额头位置，可能是比较好的解决方案。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
