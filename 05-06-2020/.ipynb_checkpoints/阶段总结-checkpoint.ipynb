{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阶段总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一阶段的学习和测试中，主要使用了openface，dlib，opencv等库中训练好的人脸识别模型，并在图片和实时视频上进行测试。同时，通过视频课程学习了基本CNN的结构（如padding，stride，pooling），ResNets对于深层CNN阶梯下降逐渐消失等问题的优化，YOLO的结构和算法概念（如IoU，Anchor Box，non-max suppression），以及相比于利用了region proposal的R-CNN，Fast/Faster R-CNN在实时运算上的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试使用多种模型确定人脸及额头位置，并标出5点/68点人脸特征\n",
    "    * 主要收获：\n",
    "        * 熟悉detector和predictor的调用、输入、输出。\n",
    "        * 通过返回的人脸特征点估算出额头的位置并用OpenCV的polylines方程画出额头区域。\n",
    "2. 测试了dlib库中训练好的HOG模型确定人脸位置并标出5点/68点人脸特征\n",
    "    * 主要收获：\n",
    "        * 通过分别测试5点和68点人脸特征的fps，意识到两个人脸特征模型在运行速度上没有较大差距。\n",
    "        * 通过比较两个模型文件大小，5点人脸特征模型的存储占用仅为68模型的1/10，从而得出5点模型更适合在移动设备上使用的结论。\n",
    "3. 测试了ultra light模型确定人脸位置并标出5点/68点人脸特征\n",
    "    * 主要收获：\n",
    "        * 通过与dlib的HOG模型对比，ultra light的准确率、可检测面部角度、检测距离、已经运行速度都明显优于HOG。\n",
    "        * 通过每n秒检测一次的方法进一步提高fps至25帧每秒。\n",
    "4. 测试了KCF人脸跟踪模型与ultra light模型的融合：每n秒检测一次人脸位置并对跟踪校正，其他时候进行人脸跟踪\n",
    "    * 主要收获：\n",
    "        * 提高了人脸在持续移动时，人脸及额头选框的准确性。\n",
    "        * 通过人脸跟踪算法来弥补非检测帧人脸移动带来的误差，可以进一步提高n值从而提高fps。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阶段性代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import time\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnx_tf.backend import prepare\n",
    "from helpers import predict, get_forehead_coord, reshape_forehead_coord\n",
    "\n",
    "import KCF\n",
    "\n",
    "onDetecting = True\n",
    "onTracking = False\n",
    "\n",
    "onnx_path = 'model/ultra_light_320.onnx'  # OPTION: model/ultra_light_640.onnx\n",
    "predictor_path = 'model/shape_predictor_5_face_landmarks.dat'\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "detector = prepare(onnx_model)\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "trackers = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# initiate loop and timer\n",
    "loop = 0\n",
    "start = time.time()\n",
    "\n",
    "while True:\n",
    "    loop += 1\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    h, w, _ = frame.shape\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # convert bgr to grey\n",
    "\n",
    "    if onDetecting:\n",
    "        # pre-process img acquired\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # convert bgr to rgb\n",
    "        img = cv2.resize(img, (320, 240))  # OPTION: 640 * 480\n",
    "        img_mean = np.array([127, 127, 127])\n",
    "        img = (img - img_mean) / 128\n",
    "        img = np.transpose(img, [2, 0, 1])\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img.astype(np.float32)\n",
    "\n",
    "        confidences, boxes = ort_session.run(None, {input_name: img})\n",
    "        boxes, labels, probs = predict(w, h, confidences, boxes, 0.7)\n",
    "\n",
    "        for i in range(boxes.shape[0]):\n",
    "            face_box = boxes[i, :]\n",
    "            bX, bY, bW, bH = face_box\n",
    "            cv2.rectangle(frame, (bX, bY), (bW, bH), (0, 255, 0), 2)\n",
    "            tracker = KCF.kcftracker(False, True, False, False)  # hog, fixed_window, multi-scale, lab\n",
    "            tracker.init([bX, bY, bW - bX, bH - bY], frame)\n",
    "            trackers.append(tracker)\n",
    "\n",
    "            face_rect = dlib.rectangle(left=bX, top=bY, right=bW, bottom=bH)\n",
    "            shape = predictor(gray, face_rect)  # get 5-point facial landmarks\n",
    "            cv2.polylines(frame, get_forehead_coord(shape), True, (0, 255, 255), 2)  # draw forehead box\n",
    "\n",
    "        # keep detecting until face found\n",
    "        if boxes.shape[0] > 0:\n",
    "            onDetecting = False\n",
    "            onTracking = True\n",
    "\n",
    "    elif onTracking:\n",
    "        for tracker in trackers:\n",
    "            face_bbox = tracker.update(frame)  # get tracked face bounding box\n",
    "            f_x1, f_y1, f_w, f_h = face_bbox[0], face_bbox[1], face_bbox[2], face_bbox[3]\n",
    "            cv2.rectangle(frame, (f_x1, f_y1), (f_x1 + f_w, f_y1 + f_h), (0, 255, 0), 2)\n",
    "\n",
    "            face_rect = dlib.rectangle(left=f_x1, top=f_y1, right=f_x1+f_w, bottom=f_y1+f_h)\n",
    "            shape = predictor(gray, face_rect)  # get 5-point facial landmarks\n",
    "            cv2.polylines(frame, get_forehead_coord(shape), True, (0, 255, 255), 2)  # draw forehead box\n",
    "\n",
    "        # run detector every 10 frames\n",
    "        if loop % 10 == 0:\n",
    "            trackers = []\n",
    "            forehead_trackers = []\n",
    "            onDetecting = True\n",
    "            onTracking = False\n",
    "\n",
    "    cv2.imshow('tracking', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # end timer\n",
    "        end = time.time()\n",
    "        print(\"Average fps: \", loop / (end - start))\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github: https://github.com/andyzihaozhang/Face-Recog-and-Track-Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 未来计划"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 寻找合适的模型并通过fine-tune将模型应用到人脸口罩识别\n",
    "    * 进展：\n",
    "        * 已找到人脸口罩数据集\n",
    "        * 已找到模型训练的[教程](https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/)，教程中使用了keras，TensorFlow等库和MobileNetV2模型（适合移动设备实时视频检测的模型）。\n",
    "2. 将注意力转移到CT肺部分割\n",
    "    * 进展：\n",
    "        * 已找到多个已经训练好并有benchmark比较的模型（方老师提供）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
